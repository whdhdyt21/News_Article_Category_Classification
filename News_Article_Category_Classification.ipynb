{"cells":[{"cell_type":"markdown","metadata":{"id":"dwltaMmfE6nS"},"source":["# **Import Library**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Beop3JayvQYH"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer # Untuk menghitung TF-IDF\n","from sklearn import preprocessing # Untuk mengubah label menjadi angka\n","import pandas as pd # Untuk membaca data\n","from sklearn.model_selection import train_test_split # Untuk membagi data menjadi data latih dan data uji\n","import nltk # Untuk pemrosesan bahasa alami\n","from nltk.corpus import stopwords # Untuk menghapus kata-kata yang tidak penting\n","from nltk.stem import WordNetLemmatizer # Untuk mengubah kata ke bentuk dasar\n","from sklearn.metrics import confusion_matrix, accuracy_score,classification_report # Untuk mengevaluasi model\n","import numpy as np # aljabar linear\n","import pandas as pd # pemrosesan data, CSV file I/O (misalnya pd.read_csv)\n","import string # untuk operasi string\n","\n","lemma = WordNetLemmatizer() # Inisialisasi WordNetLemmatizer\n","# Download data untuk pemrosesan bahasa alami\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Menghubungkan ke google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive"]},{"cell_type":"markdown","metadata":{"id":"i-5H0neIE98x"},"source":["# **Load Datasets**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKJGtM7D2jvM"},"outputs":[],"source":["data = pd.read_csv('news-article-categories.csv') # Membaca file CSV\n","df = pd.DataFrame(data) # Membuat DataFrame dari data\n","df.tail(10) # Menampilkan 10 data terakhir"]},{"cell_type":"markdown","metadata":{"id":"gTmgo0_c-5pm"},"source":["# **Removing NULL Values**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvfDUCME3-QJ"},"outputs":[],"source":["df.isnull().sum() # Digunakan untuk menghitung jumlah nilai null (NaN) dalam setiap kolom DataFrame df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwhVxiT2911u"},"outputs":[],"source":["# Digunakan untuk menghapus baris dan kolom dari DataFrame df yang mengandung setidaknya satu nilai null (NaN).\n","df = df.dropna()\n","df = df.dropna(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBvh6JVU-o0v"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"VExJcMufBwIV"},"source":["# **Encoding Category**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wePArQ__BzoQ"},"outputs":[],"source":["df['category'].value_counts() #  Untuk menghitung jumlah kemunculan setiap nilai unik dalam kolom \"category\" dari DataFrame df yang berguna untuk melihat distribusi kategori berita dalam data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIG7VCZrFyYV"},"outputs":[],"source":["# Untuk mengonversi nilai kategori dalam kolom \"category\" menjadi angka\n","label_encoder = preprocessing.LabelEncoder()\n","label_encoder.fit(df['category'])\n","df['label'] = label_encoder.transform(df['category'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ilSXigYfiGY"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"RP1rHekQrMrT"},"source":["# **Text Preperation**"]},{"cell_type":"markdown","metadata":{"id":"Dhn0ZkZFrvOE"},"source":["# **Lower Case**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gFXndcnrp9a"},"outputs":[],"source":["# Untuk mengubah semua teks dalam kolom \"body\" dari DataFrame df menjadi huruf kecil (lowercase)\n","df['body']=df['body'].str.lower()\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"h6ZtAeRfsYEi"},"source":["# **Remove HTML Tags**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwlGcDB4siqI"},"outputs":[],"source":["# Import modul BeautifulSoup\n","from bs4 import BeautifulSoup\n","\n","# Mengecek apakah ada tag html didalam teks\n","\n","# Mendefinisikan Fungsi 'has_html_tags'\n","def has_html_tags(text):\n","    soup = BeautifulSoup(text, 'html.parser')\n","    return bool(soup.find())\n","\n","df['has_html_tags'] = df['body'].apply(has_html_tags)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JjYoTFMmvFEb"},"outputs":[],"source":["count_true = df['has_html_tags'].sum()\n","count_true # jumlah total baris di mana tag HTML ditemukan dalam DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1npNEJFhwsWk"},"outputs":[],"source":["df = df.drop(df[df['has_html_tags']].index) # mengembalikan baris-baris di mana tag HTML ditemukan dalam teks pada kolom \"body\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eYEgHO-387og"},"outputs":[],"source":["# Untuk menghapus kolom 'has_html_tags' dari DataFrame df\n","df = df.drop('has_html_tags', axis=1)\n","df"]},{"cell_type":"markdown","metadata":{"id":"dU0W47Yu9W6E"},"source":["# **Remove Emojies**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XKOgX1Mk-RIA"},"outputs":[],"source":["import regex # Mengimpor modul regex yang digunakan untuk bekerja dengan ekspresi reguler yang mendukung Unicode\n","\n","# Mendefinisikan Fungsi 'has_emoji'\n","def has_emoji(text):\n","    emoji_pattern = regex.compile(r'\\p{Emoji}', flags=regex.UNICODE)\n","    return bool(emoji_pattern.search(text))\n","\n","\n","has_emojis =  df['body'].apply(has_emoji)\n","\n","has_emojis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fGObGR5-cd5"},"outputs":[],"source":["has_emojis.sum() #untuk menghitung jumlah nilai True dalam has_emojis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUjNvn07-uJ1"},"outputs":[],"source":["# Mendefinisikan fungsi remove_emojis\n","def remove_emojis(text):\n","    emoji_pattern = regex.compile(r'\\p{Emoji}', flags=regex.UNICODE)\n","    return emoji_pattern.sub('', text)\n","\n","df['body'] = df['body'].apply(remove_emojis)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92nHeW9w_A44"},"outputs":[],"source":["# Untuk mengecek apakah setiap baris dalam kolom \"body\" yang sudah menggunakan metode apply dari DataFrame df mengandung emoji atau tidak\n","# (True jika emoji ditemukan, False jika tidak)\n","has_emojis =  df['body'].apply(has_emoji)\n","has_emojis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtX-iGuL_JTS"},"outputs":[],"source":["has_emojis.sum() #untuk menghitung jumlah nilai True dalam has_emojis"]},{"cell_type":"markdown","metadata":{"id":"LUDo8Rgr_7NS"},"source":["# **Remove urls**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8oJJ-hSAGJY"},"outputs":[],"source":["import re # Mengimpor modul re yang digunakan untuk bekerja dengan ekspresi reguler (regular expressions) di Python.\n","\n","# Mendefinisikan fungsi 'remove_url'\n","def remove_url(text):\n","    pattern=re.compile(r'https?://\\S+|www\\.S+')\n","    return pattern.sub(r'',text)\n","df[\"body\"]=df[\"body\"].apply(remove_url)"]},{"cell_type":"markdown","metadata":{"id":"KyS0QguaNMxE"},"source":["# **Tokenisasi, Remove punctuation, Remove stopwords**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZ8CFQ2lNQB1"},"outputs":[],"source":["# Impor library\n","from string import punctuation\n","\n","string.punctuation\n","def DataPrep(text) :\n","    # Tokenisasi\n","    tokens = nltk.word_tokenize(text)\n","\n","    # Menghapus tanda baca\n","    punc = list(punctuation)\n","    words = [w for w in tokens if w not in punc]\n","\n","    # Menghapus stopwords\n","    stop_words = set(stopwords.words('english'))\n","\n","    # lemmatization\n","    words = [lemma.lemmatize(w) for w in words]\n","\n","    text = ' '.join(words)\n","\n","    return text"]},{"cell_type":"markdown","metadata":{"id":"yOYG0NOgSOPi"},"source":["# **TF-IDF Vectorizer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irRSyFa3SYQ4"},"outputs":[],"source":["x = df['body'] = df['body'].apply(DataPrep) # Menggunakan kolom \"body\" dari dataframe df dan menerapkannya ke fungsi DataPrep menggunakan metode apply.\n","y = df['label']  # Menggunakan kolom \"label\" sebagai target\n","\n","# Vektorisasi teks menggunakan TF-IDF\n","vectorizer = TfidfVectorizer(stop_words = 'english', max_features=6000)\n","vectorized_x = vectorizer.fit_transform(x.values.astype('U'))"]},{"cell_type":"markdown","metadata":{"id":"fUOqePpqss01"},"source":["# **Split**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMgcfNzKswG-"},"outputs":[],"source":["# Asumsikan fitur vectorized_x sebagai fitur dan 'y' merupakan vektor label\n","# Gunakan stratify untuk memastikan bahwa pembagian data dilakukan dengan mempertahankan distribusi kelas yang sama seperti di dataset asli.\n","x_train, x_test, y_train, y_test = train_test_split(vectorized_x, y, test_size=0.1, stratify=y, random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"bPyI3vq3Icot"},"source":["# **Smote**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQBdtmcVIfDn"},"outputs":[],"source":["# Impor library\n","from imblearn.over_sampling import SMOTE\n","\n","# Ganti Random Over Sampling dengan SMOTE\n","smote = SMOTE()\n","X_resampled, y_resampled = smote.fit_resample(x_train, y_train)\n","\n","# Buat DataFrame baru\n","os = pd.DataFrame(list(zip([x[0] for x in X_resampled], y_resampled)), columns=['body', 'Label'])\n","\n","# Tampilkan distribusi kelas setelah penerapan SMOTE\n","os['Label'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"HgnBLd_DPHUr"},"source":["# **Model**"]},{"cell_type":"markdown","metadata":{"id":"5hSqMlTG_UPe"},"source":["# **SVM**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwQqUajzxTyc"},"outputs":[],"source":["# Eka Pramuditya\n","# Mengimpor library yang dibutuhkan\n","from sklearn.svm import SVC\n","\n","# Membuat model\n","classifier = SVC(random_state=42)\n","\n","# Pelatihan model\n","classifier.fit(X_resampled, y_resampled)\n","\n","# Prediksi pada data pengujian (x_test)\n","y_pred = classifier.predict(x_test)\n","\n","# Evaluasi model\n","SVM_ACCURACY = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"S1eCaQ66hbEN"},"source":["# **Logistic Regression**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxLqTY8shaN1"},"outputs":[],"source":["# Angger Haryo Putranto\n","# Mengimpor library yang dibutuhkan\n","from sklearn.linear_model import LogisticRegression\n","\n","# Membuat model\n","model = LogisticRegression(random_state=42, max_iter=200)\n","\n","# Pelatihan model\n","model.fit(X_resampled, y_resampled)\n","\n","# Prediksi pada data pengujian (x_test)\n","y_pred = model.predict(x_test)\n","\n","# Evaluasi model\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","print(\"Classification Report:\")\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"LnZxHe9cfEAo"},"source":["# **Stochastic Gradient Descent**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-ph3G5cfK-v"},"outputs":[],"source":["# Wahid Hidayat\n","\n","from sklearn.linear_model import SGDClassifier # Mengimpor SGDClassifier\n","\n","classifier = SGDClassifier(random_state=0) # Menggunakan SGDClassifier dengan random_state=0\n","\n","classifier.fit(X_resampled, y_resampled) # Melatih model dengan data latih yang sudah di-resampling\n","\n","# Prediksi pada data pengujian (x_test)\n","y_pred = classifier.predict(x_test) # Melakukan prediksi pada data pengujian\n","\n","# Evaluasi model\n","SGD_ACCURACY = accuracy_score(y_test, y_pred) # Menghitung akurasi model\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred)) # Menampilkan akurasi model\n","\n","cm = confusion_matrix(y_test, y_pred) # Menghitung confusion matrix\n","print(cm) # Menampilkan confusion matrix\n","\n","print(\"Classification Report:\") # Menampilkan classification report\n","print(classification_report(y_test, y_pred)) # Menampilkan classification report dari model"]},{"cell_type":"markdown","metadata":{"id":"C4g4CwN59qC0"},"source":["#**Ensemble Stacking Classifier With SVM, Decision Tree, and logistic regression**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOaBnmja9oxF"},"outputs":[],"source":["# Farhan Husyen Ramadhan\n","# Mengimpor library yang dibutuhkan\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","# Membuat model\n","model1 = SVC(random_state=42)\n","model2 = DecisionTreeClassifier(random_state=0)\n","model3 = LogisticRegression(random_state=0)\n","\n","# Membuat StackingClassifier\n","ensembleSC = StackingClassifier(estimators=[('svc', model1), ('dt', model2), ('lr', model3)], final_estimator=SVC())\n","\n","# Pelatihan model\n","ensembleSC.fit(X_resampled, y_resampled)\n","\n","# Evaluasi model pada data pelatihan\n","print(f\"Akurasi training: {ensembleSC.score(x_train, y_train)}\")\n","\n","# Evaluasi model pada data pengujian\n","y_pred = ensembleSC.predict(x_test)\n","print(f\"Akurasi prediksi: {accuracy_score(y_test, y_pred)}\")\n","print(classification_report(y_test, y_pred))"]}],"metadata":{"colab":{"collapsed_sections":["dwltaMmfE6nS","gTmgo0_c-5pm","VExJcMufBwIV","Dhn0ZkZFrvOE","h6ZtAeRfsYEi","dU0W47Yu9W6E","LUDo8Rgr_7NS"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}